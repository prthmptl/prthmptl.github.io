<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Blogs on Pratham Patel</title><link>https://prthmptl.github.io/blog/</link><description>Recent content in Blogs on Pratham Patel</description><generator>Hugo -- 0.143.1</generator><language>en-us</language><lastBuildDate>Sun, 16 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://prthmptl.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>2D Matrix Multiplication with CUDA</title><link>https://prthmptl.github.io/blog/matmulkernel/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://prthmptl.github.io/blog/matmulkernel/</guid><description>&lt;h1 id="2d-matrix-multiplication-with-cuda">2D Matrix Multiplication with CUDA&lt;/h1>
&lt;p>Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we&amp;rsquo;ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.&lt;/p>
&lt;h2 id="why-use-cuda-for-matrix-multiplication">Why Use CUDA for Matrix Multiplication?&lt;/h2>
&lt;p>Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities.&lt;/p></description></item><item><title>Understanding CUDA: A Simple Vector Addition Example</title><link>https://prthmptl.github.io/blog/vecaddkernel/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://prthmptl.github.io/blog/vecaddkernel/</guid><description>&lt;h1 id="understanding-cuda-a-simple-vector-addition-example">Understanding CUDA: A Simple Vector Addition Example&lt;/h1>
&lt;p>CUDA (Compute Unified Device Architecture) is a parallel computing platform by NVIDIA that allows developers to leverage the massive computational power of GPUs. In this blog, we&amp;rsquo;ll break down a simple CUDA program that performs vector addition using GPU acceleration.&lt;/p>
&lt;h2 id="the-code">The Code&lt;/h2>
&lt;p>Let&amp;rsquo;s analyze the given CUDA program, which adds two vectors element-wise using parallel processing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-cpp" data-lang="cpp">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;iostream&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;cuda_runtime.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__global__
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">vecAddKernel&lt;/span>(&lt;span style="color:#66d9ef">float&lt;/span>&lt;span style="color:#f92672">*&lt;/span> A, &lt;span style="color:#66d9ef">float&lt;/span>&lt;span style="color:#f92672">*&lt;/span> B, &lt;span style="color:#66d9ef">float&lt;/span>&lt;span style="color:#f92672">*&lt;/span> C, &lt;span style="color:#66d9ef">int&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> i &lt;span style="color:#f92672">=&lt;/span> threadIdx.x &lt;span style="color:#f92672">+&lt;/span> blockDim.x &lt;span style="color:#f92672">*&lt;/span> blockIdx.x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> (i &lt;span style="color:#f92672">&amp;lt;&lt;/span> n) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> C[i] &lt;span style="color:#f92672">=&lt;/span> A[i] &lt;span style="color:#f92672">+&lt;/span> B[i];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">int&lt;/span> &lt;span style="color:#a6e22e">main&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">const&lt;/span> &lt;span style="color:#66d9ef">int&lt;/span> n &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">256&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">float&lt;/span> A[n], B[n], C[n];
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> i&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>; i&lt;span style="color:#f92672">&amp;lt;=&lt;/span>n; i&lt;span style="color:#f92672">++&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> A[i&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> i;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> B[i&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> i;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">float&lt;/span> &lt;span style="color:#f92672">*&lt;/span>A_d, &lt;span style="color:#f92672">*&lt;/span>B_d, &lt;span style="color:#f92672">*&lt;/span>C_d;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> size &lt;span style="color:#f92672">=&lt;/span> n &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#66d9ef">sizeof&lt;/span>(&lt;span style="color:#66d9ef">float&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaMalloc((&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#f92672">**&lt;/span>) &lt;span style="color:#f92672">&amp;amp;&lt;/span>A_d, size);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaMalloc((&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#f92672">**&lt;/span>) &lt;span style="color:#f92672">&amp;amp;&lt;/span>B_d, size);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaMalloc((&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#f92672">**&lt;/span>) &lt;span style="color:#f92672">&amp;amp;&lt;/span>C_d, size);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaMemcpy(A_d, A, size, cudaMemcpyHostToDevice);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaMemcpy(B_d, B, size, cudaMemcpyHostToDevice);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> vecAddKernel&lt;span style="color:#f92672">&amp;lt;&amp;lt;&amp;lt;&lt;/span>ceil(n&lt;span style="color:#f92672">/&lt;/span>&lt;span style="color:#ae81ff">256.0&lt;/span>), &lt;span style="color:#ae81ff">256&lt;/span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span>(A_d, B_d, C_d, n);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaMemcpy(C, C_d, size, cudaMemcpyDeviceToHost);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> i&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">0&lt;/span>; i&lt;span style="color:#f92672">&amp;lt;&lt;/span>n; i&lt;span style="color:#f92672">++&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> printf(&lt;span style="color:#e6db74">&amp;#34;%.2f&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>, C[i]);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaFree(A_d);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaFree(B_d);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cudaFree(C_d);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="breaking-it-down">Breaking It Down&lt;/h2>
&lt;h3 id="1-the-kernel-function">1. The Kernel Function&lt;/h3>
&lt;p>The function &lt;code>vecAddKernel&lt;/code> is a CUDA kernel, which means it runs on the GPU. It follows this format:&lt;/p></description></item></channel></rss>