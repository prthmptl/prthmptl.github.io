<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2D Matrix Multiplication with CUDA | Pratham Patel</title>
<meta name=keywords content><meta name=description content="2D Matrix Multiplication with CUDA
Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we&rsquo;ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.
Why Use CUDA for Matrix Multiplication?
Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities."><meta name=author content="Pratham Patel"><link rel=canonical href=https://prthmptl.github.io/blog/matmulkernel/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://prthmptl.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://prthmptl.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://prthmptl.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://prthmptl.github.io/apple-touch-icon.png><link rel=mask-icon href=https://prthmptl.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://prthmptl.github.io/blog/matmulkernel/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://prthmptl.github.io/blog/matmulkernel/"><meta property="og:site_name" content="Pratham Patel"><meta property="og:title" content="2D Matrix Multiplication with CUDA"><meta property="og:description" content="2D Matrix Multiplication with CUDA Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we’ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.
Why Use CUDA for Matrix Multiplication? Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-02-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-16T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="2D Matrix Multiplication with CUDA"><meta name=twitter:description content="2D Matrix Multiplication with CUDA
Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we&rsquo;ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.
Why Use CUDA for Matrix Multiplication?
Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://prthmptl.github.io/blog/"},{"@type":"ListItem","position":2,"name":"2D Matrix Multiplication with CUDA","item":"https://prthmptl.github.io/blog/matmulkernel/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2D Matrix Multiplication with CUDA","name":"2D Matrix Multiplication with CUDA","description":"2D Matrix Multiplication with CUDA Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we\u0026rsquo;ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.\nWhy Use CUDA for Matrix Multiplication? Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities.\n","keywords":[],"articleBody":"2D Matrix Multiplication with CUDA Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we’ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.\nWhy Use CUDA for Matrix Multiplication? Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities.\nUnderstanding the Code Step by Step This C++ program with CUDA performs matrix multiplication using GPU acceleration. Let’s break it down logically.\n1. Including Required Headers #include #include We include for input-output operations and to use CUDA functions that facilitate GPU computations.\n2. CUDA Kernel for Matrix Multiplication __global__ void matMulKernel(float* A, float* B, float* C, int n) { int i = threadIdx.x + blockDim.x * blockIdx.x; // Row index int j = threadIdx.y + blockDim.y * blockIdx.y; // Column index float sum = 0; if(i\u003cn \u0026\u0026 j\u003cn) { for (int k = 0; k \u003c n; k++) sum += A[i * n + k] * B[k * n + j]; C[i * n + j] = sum; } } This kernel function is executed on the GPU. Each thread in CUDA is responsible for computing one element of the result matrix C.\nBreaking Down the CUDA Kernel with an Example CUDA kernels execute in parallel, meaning that each thread operates on a specific element of the output matrix C. Let’s break it down using an example.\nExample Matrix Multiplication Suppose we have two small matrices:\nA = | 1 2 | B = | 3 4 | | 3 4 | | 5 6 | The expected result C = A × B is:\nC = | (1×3 + 2×5) (1×4 + 2×6) | = | 13 16 | | (3×3 + 4×5) (3×4 + 4×6) | | 29 36 | How CUDA Threads Process This Computation Each thread computes a single element of C. Let’s assume a 2×2 grid of threads.\nThread (0,0) computes C[0][0]: i = 0 (row 0), j = 0 (column 0) Calculation: 1×3 + 2×5 = 13 Thread (0,1) computes C[0][1]: i = 0 (row 0), j = 1 (column 1) Calculation: 1×4 + 2×6 = 16 Thread (1,0) computes C[1][0]: i = 1 (row 1), j = 0 (column 0) Calculation: 3×3 + 4×5 = 29 Thread (1,1) computes C[1][1]: i = 1 (row 1), j = 1 (column 1) Calculation: 3×4 + 4×6 = 36 Each thread retrieves the corresponding row from A and column from B and performs the dot product calculation independently. Since all threads execute in parallel, the computation completes much faster than sequential execution on a CPU.\nExplanation of CUDA Thread Indexing int i = threadIdx.x + blockDim.x * blockIdx.x; // i represents row index int j = threadIdx.y + blockDim.y * blockIdx.y; // j represents column index Each thread gets a unique (i, j) pair, which determines which element of C it calculates.\nUnderstanding Index Computation A[i * n + k] retrieves the element at row i and column k in A. B[k * n + j] retrieves the element at row k and column j in B. The final computed C[i * n + j] is stored in the correct position in C. For example, for i = 0, j = 1:\nThe kernel fetches A[0 * 2 + k] for k = 0, 1, which corresponds to row 0 in A. It fetches B[k * 2 + 1], which corresponds to column 1 in B. The sum of the dot product is stored in C[0 * 2 + 1], which is C[0][1]. This indexing system allows us to scale up the matrix multiplication to larger sizes by distributing computations across multiple threads and blocks.\n3. Main Function: Memory Allocation and Execution int main() { const int n = 10; float A[n][n], B[n][n], C[n][n]; We define the size of matrices and allocate space in host (CPU) memory.\nInitializing Matrices for(int i=0; i\u003cn; i++) { for(int j=0; j\u003cn; j++) { A[i][j] = 1.0f; B[i][j] = 2.0f; } } We initialize matrix A with 1.0 and matrix B with 2.0. This ensures predictable output, making debugging easier.\nCopying Data to GPU cudaMemcpy(A_d, A, size, cudaMemcpyHostToDevice); cudaMemcpy(B_d, B, size, cudaMemcpyHostToDevice); Using cudaMemcpy, we transfer the matrix data from CPU to GPU memory.\n4. Launching the CUDA Kernel dim3 blocksPerGrid((2*n-1)/n, (2*n-1)/n); dim3 threadsPerBlock(16, 16); matMulKernel\u003c\u003c\u003cblocksPerGrid, threadsPerBlock\u003e\u003e\u003e(A_d, B_d, C_d, n); 5. Copying Results Back to CPU and Printing cudaMemcpy(C, C_d, size, cudaMemcpyDeviceToHost); 6. Freeing Memory cudaFree(A_d); cudaFree(B_d); cudaFree(C_d); Conclusion This program demonstrates how to implement matrix multiplication using CUDA for parallel execution on the GPU. Understanding thread indexing, memory allocation, and kernel execution is key to writing efficient CUDA programs. By experimenting with different optimizations, we can further enhance performance and scalability.\n","wordCount":"839","inLanguage":"en","datePublished":"2025-02-16T00:00:00Z","dateModified":"2025-02-16T00:00:00Z","author":{"@type":"Person","name":"Pratham Patel"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://prthmptl.github.io/blog/matmulkernel/"},"publisher":{"@type":"Organization","name":"Pratham Patel","logo":{"@type":"ImageObject","url":"https://prthmptl.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://prthmptl.github.io/ accesskey=h title="Pratham Patel (Alt + H)">Pratham Patel</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://prthmptl.github.io/about/ title=About><span>About</span></a></li><li><a href=https://prthmptl.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://prthmptl.github.io/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://prthmptl.github.io/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">2D Matrix Multiplication with CUDA</h1><div class=post-meta><span title='2025-02-16 00:00:00 +0000 UTC'>February 16, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Pratham Patel</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#2d-matrix-multiplication-with-cuda aria-label="2D Matrix Multiplication with CUDA">2D Matrix Multiplication with CUDA</a><ul><li><a href=#why-use-cuda-for-matrix-multiplication aria-label="Why Use CUDA for Matrix Multiplication?">Why Use CUDA for Matrix Multiplication?</a></li><li><a href=#understanding-the-code-step-by-step aria-label="Understanding the Code Step by Step">Understanding the Code Step by Step</a><ul><li><a href=#1-including-required-headers aria-label="1. Including Required Headers">1. Including Required Headers</a></li><li><a href=#2-cuda-kernel-for-matrix-multiplication aria-label="2. CUDA Kernel for Matrix Multiplication">2. CUDA Kernel for Matrix Multiplication</a></li><li><a href=#breaking-down-the-cuda-kernel-with-an-example aria-label="Breaking Down the CUDA Kernel with an Example">Breaking Down the CUDA Kernel with an Example</a><ul><li><a href=#example-matrix-multiplication aria-label="Example Matrix Multiplication">Example Matrix Multiplication</a></li></ul></li><li><a href=#how-cuda-threads-process-this-computation aria-label="How CUDA Threads Process This Computation">How CUDA Threads Process This Computation</a></li><li><a href=#explanation-of-cuda-thread-indexing aria-label="Explanation of CUDA Thread Indexing">Explanation of CUDA Thread Indexing</a><ul><li><a href=#understanding-index-computation aria-label="Understanding Index Computation">Understanding Index Computation</a></li></ul></li><li><a href=#3-main-function-memory-allocation-and-execution aria-label="3. Main Function: Memory Allocation and Execution">3. Main Function: Memory Allocation and Execution</a><ul><li><a href=#initializing-matrices aria-label="Initializing Matrices">Initializing Matrices</a></li><li><a href=#copying-data-to-gpu aria-label="Copying Data to GPU">Copying Data to GPU</a></li></ul></li><li><a href=#4-launching-the-cuda-kernel aria-label="4. Launching the CUDA Kernel">4. Launching the CUDA Kernel</a></li><li><a href=#5-copying-results-back-to-cpu-and-printing aria-label="5. Copying Results Back to CPU and Printing">5. Copying Results Back to CPU and Printing</a></li><li><a href=#6-freeing-memory aria-label="6. Freeing Memory">6. Freeing Memory</a></li></ul></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=2d-matrix-multiplication-with-cuda>2D Matrix Multiplication with CUDA<a hidden class=anchor aria-hidden=true href=#2d-matrix-multiplication-with-cuda>#</a></h1><p>Matrix multiplication is a fundamental operation in scientific computing, machine learning, and computer graphics. Leveraging the parallel processing power of GPUs can significantly speed up matrix operations. In this blog, we&rsquo;ll break down a CUDA-based matrix multiplication program step by step, explaining it in an intuitive manner.</p><h2 id=why-use-cuda-for-matrix-multiplication>Why Use CUDA for Matrix Multiplication?<a hidden class=anchor aria-hidden=true href=#why-use-cuda-for-matrix-multiplication>#</a></h2><p>Matrix multiplication involves many repeated calculations that can be performed in parallel. CPUs process computations sequentially for the most part, whereas GPUs excel at handling thousands of parallel operations. CUDA allows us to write programs that run efficiently on NVIDIA GPUs, leveraging their parallel computing capabilities.</p><h2 id=understanding-the-code-step-by-step>Understanding the Code Step by Step<a hidden class=anchor aria-hidden=true href=#understanding-the-code-step-by-step>#</a></h2><p>This C++ program with CUDA performs matrix multiplication using GPU acceleration. Let&rsquo;s break it down logically.</p><h3 id=1-including-required-headers>1. Including Required Headers<a hidden class=anchor aria-hidden=true href=#1-including-required-headers>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;iostream&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;cuda_runtime.h&gt;</span><span style=color:#75715e>
</span></span></span></code></pre></div><p>We include <code>&lt;iostream></code> for input-output operations and <code>&lt;cuda_runtime.h></code> to use CUDA functions that facilitate GPU computations.</p><h3 id=2-cuda-kernel-for-matrix-multiplication>2. CUDA Kernel for Matrix Multiplication<a hidden class=anchor aria-hidden=true href=#2-cuda-kernel-for-matrix-multiplication>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>__global__
</span></span><span style=display:flex><span><span style=color:#66d9ef>void</span> <span style=color:#a6e22e>matMulKernel</span>(<span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> A, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> B, <span style=color:#66d9ef>float</span><span style=color:#f92672>*</span> C, <span style=color:#66d9ef>int</span> n) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> threadIdx.x <span style=color:#f92672>+</span> blockDim.x <span style=color:#f92672>*</span> blockIdx.x; <span style=color:#75715e>// Row index
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> threadIdx.y <span style=color:#f92672>+</span> blockDim.y <span style=color:#f92672>*</span> blockIdx.y; <span style=color:#75715e>// Column index
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>float</span> sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span>(i<span style=color:#f92672>&lt;</span>n <span style=color:#f92672>&amp;&amp;</span> j<span style=color:#f92672>&lt;</span>n) {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>int</span> k <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; k <span style=color:#f92672>&lt;</span> n; k<span style=color:#f92672>++</span>)
</span></span><span style=display:flex><span>            sum <span style=color:#f92672>+=</span> A[i <span style=color:#f92672>*</span> n <span style=color:#f92672>+</span> k] <span style=color:#f92672>*</span> B[k <span style=color:#f92672>*</span> n <span style=color:#f92672>+</span> j];
</span></span><span style=display:flex><span>        C[i <span style=color:#f92672>*</span> n <span style=color:#f92672>+</span> j] <span style=color:#f92672>=</span> sum;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This kernel function is executed on the GPU. Each thread in CUDA is responsible for computing one element of the result matrix <code>C</code>.</p><h3 id=breaking-down-the-cuda-kernel-with-an-example>Breaking Down the CUDA Kernel with an Example<a hidden class=anchor aria-hidden=true href=#breaking-down-the-cuda-kernel-with-an-example>#</a></h3><p>CUDA kernels execute in parallel, meaning that each thread operates on a specific element of the output matrix <code>C</code>. Let&rsquo;s break it down using an example.</p><h4 id=example-matrix-multiplication>Example Matrix Multiplication<a hidden class=anchor aria-hidden=true href=#example-matrix-multiplication>#</a></h4><p>Suppose we have two small matrices:</p><pre tabindex=0><code>A = | 1 2 |    B = | 3 4 |
    | 3 4 |        | 5 6 |
</code></pre><p>The expected result <code>C = A × B</code> is:</p><pre tabindex=0><code>C = | (1×3 + 2×5)  (1×4 + 2×6) |  =  | 13 16 |
    | (3×3 + 4×5)  (3×4 + 4×6) |     | 29 36 |
</code></pre><h3 id=how-cuda-threads-process-this-computation>How CUDA Threads Process This Computation<a hidden class=anchor aria-hidden=true href=#how-cuda-threads-process-this-computation>#</a></h3><p>Each thread computes a single element of <code>C</code>. Let&rsquo;s assume a 2×2 grid of threads.</p><ul><li><strong>Thread (0,0)</strong> computes <code>C[0][0]</code>:<ul><li><code>i = 0</code> (row 0), <code>j = 0</code> (column 0)</li><li>Calculation: <code>1×3 + 2×5 = 13</code></li></ul></li><li><strong>Thread (0,1)</strong> computes <code>C[0][1]</code>:<ul><li><code>i = 0</code> (row 0), <code>j = 1</code> (column 1)</li><li>Calculation: <code>1×4 + 2×6 = 16</code></li></ul></li><li><strong>Thread (1,0)</strong> computes <code>C[1][0]</code>:<ul><li><code>i = 1</code> (row 1), <code>j = 0</code> (column 0)</li><li>Calculation: <code>3×3 + 4×5 = 29</code></li></ul></li><li><strong>Thread (1,1)</strong> computes <code>C[1][1]</code>:<ul><li><code>i = 1</code> (row 1), <code>j = 1</code> (column 1)</li><li>Calculation: <code>3×4 + 4×6 = 36</code></li></ul></li></ul><p>Each thread retrieves the corresponding row from <code>A</code> and column from <code>B</code> and performs the dot product calculation independently. Since all threads execute in parallel, the computation completes much faster than sequential execution on a CPU.</p><h3 id=explanation-of-cuda-thread-indexing>Explanation of CUDA Thread Indexing<a hidden class=anchor aria-hidden=true href=#explanation-of-cuda-thread-indexing>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> threadIdx.x <span style=color:#f92672>+</span> blockDim.x <span style=color:#f92672>*</span> blockIdx.x; <span style=color:#75715e>// i represents row index
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>int</span> j <span style=color:#f92672>=</span> threadIdx.y <span style=color:#f92672>+</span> blockDim.y <span style=color:#f92672>*</span> blockIdx.y; <span style=color:#75715e>// j represents column index
</span></span></span></code></pre></div><p>Each thread gets a unique <code>(i, j)</code> pair, which determines which element of <code>C</code> it calculates.</p><h4 id=understanding-index-computation>Understanding Index Computation<a hidden class=anchor aria-hidden=true href=#understanding-index-computation>#</a></h4><ul><li><code>A[i * n + k]</code> retrieves the element at row <code>i</code> and column <code>k</code> in <code>A</code>.</li><li><code>B[k * n + j]</code> retrieves the element at row <code>k</code> and column <code>j</code> in <code>B</code>.</li><li>The final computed <code>C[i * n + j]</code> is stored in the correct position in <code>C</code>.</li></ul><p>For example, for <code>i = 0</code>, <code>j = 1</code>:</p><ul><li>The kernel fetches <code>A[0 * 2 + k]</code> for <code>k = 0, 1</code>, which corresponds to row <code>0</code> in <code>A</code>.</li><li>It fetches <code>B[k * 2 + 1]</code>, which corresponds to column <code>1</code> in <code>B</code>.</li><li>The sum of the dot product is stored in <code>C[0 * 2 + 1]</code>, which is <code>C[0][1]</code>.</li></ul><p>This indexing system allows us to scale up the matrix multiplication to larger sizes by distributing computations across multiple threads and blocks.</p><h3 id=3-main-function-memory-allocation-and-execution>3. Main Function: Memory Allocation and Execution<a hidden class=anchor aria-hidden=true href=#3-main-function-memory-allocation-and-execution>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>int</span> n <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>float</span> A[n][n], B[n][n], C[n][n];
</span></span></code></pre></div><p>We define the size of matrices and allocate space in host (CPU) memory.</p><h4 id=initializing-matrices>Initializing Matrices<a hidden class=anchor aria-hidden=true href=#initializing-matrices>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>int</span> i<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; i<span style=color:#f92672>&lt;</span>n; i<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>int</span> j<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>; j<span style=color:#f92672>&lt;</span>n; j<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>        A[i][j] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0f</span>;
</span></span><span style=display:flex><span>        B[i][j] <span style=color:#f92672>=</span> <span style=color:#ae81ff>2.0f</span>;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>We initialize matrix <code>A</code> with <code>1.0</code> and matrix <code>B</code> with <code>2.0</code>. This ensures predictable output, making debugging easier.</p><h4 id=copying-data-to-gpu>Copying Data to GPU<a hidden class=anchor aria-hidden=true href=#copying-data-to-gpu>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>cudaMemcpy(A_d, A, size, cudaMemcpyHostToDevice);
</span></span><span style=display:flex><span>cudaMemcpy(B_d, B, size, cudaMemcpyHostToDevice);
</span></span></code></pre></div><p>Using <code>cudaMemcpy</code>, we transfer the matrix data from CPU to GPU memory.</p><h3 id=4-launching-the-cuda-kernel>4. Launching the CUDA Kernel<a hidden class=anchor aria-hidden=true href=#4-launching-the-cuda-kernel>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>dim3 <span style=color:#a6e22e>blocksPerGrid</span>((<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>n<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>/</span>n, (<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>n<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>/</span>n);
</span></span><span style=display:flex><span>dim3 <span style=color:#a6e22e>threadsPerBlock</span>(<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>16</span>);
</span></span><span style=display:flex><span>matMulKernel<span style=color:#f92672>&lt;&lt;&lt;</span>blocksPerGrid, threadsPerBlock<span style=color:#f92672>&gt;&gt;&gt;</span>(A_d, B_d, C_d, n);
</span></span></code></pre></div><h3 id=5-copying-results-back-to-cpu-and-printing>5. Copying Results Back to CPU and Printing<a hidden class=anchor aria-hidden=true href=#5-copying-results-back-to-cpu-and-printing>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>cudaMemcpy(C, C_d, size, cudaMemcpyDeviceToHost);
</span></span></code></pre></div><h3 id=6-freeing-memory>6. Freeing Memory<a hidden class=anchor aria-hidden=true href=#6-freeing-memory>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>cudaFree(A_d);
</span></span><span style=display:flex><span>cudaFree(B_d);
</span></span><span style=display:flex><span>cudaFree(C_d);
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This program demonstrates how to implement matrix multiplication using CUDA for parallel execution on the GPU. Understanding thread indexing, memory allocation, and kernel execution is key to writing efficient CUDA programs. By experimenting with different optimizations, we can further enhance performance and scalability.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://prthmptl.github.io/>Pratham Patel</a></span> ·
</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>